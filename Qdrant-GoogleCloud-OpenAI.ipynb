{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Text from PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "import numpy as np\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    reader = PdfReader(pdf_path)\n",
    "    extracted_text = \"\"\n",
    "    for page in reader.pages:\n",
    "        extracted_text += page.extract_text()\n",
    "    return extracted_text\n",
    "\n",
    "def extract_text_from_pdfs_in_directory(directory):\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(directory, filename)\n",
    "            extracted_text = extract_text_from_pdf(pdf_path)\n",
    "            txt_filename = os.path.splitext(filename)[0] + \".txt\"\n",
    "            txt_filepath = os.path.join(directory, txt_filename)\n",
    "            with open(txt_filepath, \"w\") as txt_file:\n",
    "                txt_file.write(extracted_text)\n",
    "\n",
    "# Specify the directory containing PDF files\n",
    "directory_path = \"Docs/\"\n",
    "\n",
    "# Extract text from PDFs in the directory and save as text files\n",
    "extract_text_from_pdfs_in_directory(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Claude 3 Model Family: Opus, Sonnet, Haiku\\nAnthropic\\nAbstract\\nWe introduce Claude 3, a new family of large multimodal models â€“ Claude 3 Opus , our\\nmost capable offering, Claude 3 Sonnet , which provides a combination of skills and speed,\\nandClaude 3 Haiku , our fastest and least expensive model.', 'All new models have vision\\ncapabilities that enable them to process and analyze image data.', 'The Claude 3 family\\ndemonstrates strong performance across benchmark evaluations and sets a new standard on\\nmeasures of reasoning, math, and coding.', 'Claude 3 Opus achieves state-of-the-art results\\non evaluations like GPQA [1], MMLU [2], MMMU [3] and many more.', 'Claude 3 Haiku\\nperforms as well or better than Claude 2 [4] on most pure-text tasks, while Sonnet and\\nOpus significantly outperform it.', 'Additionally, these models exhibit improved fluency in\\nnon-English languages, making them more versatile for a global audience.', 'In this report,\\nwe provide an in-depth analysis of our evaluations, focusing on core capabilities, safety,\\nsocietal impacts, and the catastrophic risk assessments we committed to in our Responsible\\nScaling Policy [5].', '1 Introduction\\nThis model card introduces the Claude 3 family of models, which set new industry benchmarks across rea-\\nsoning, math, coding, multi-lingual understanding, and vision quality.', 'Like its predecessors, Claude 3 models employ various training methods, such as unsupervised learning and\\nConstitutional AI [6].', 'These models were trained using hardware from Amazon Web Services (AWS) and\\nGoogle Cloud Platform (GCP), with core frameworks including PyTorch [7], JAX [8], and Triton [9].']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "directory_path = \"Docs\"\n",
    "\n",
    "# List all .txt files in the directory\n",
    "txt_files = [file for file in os.listdir(directory_path) if file.endswith('.txt')]\n",
    "\n",
    "# List to store sentences from all files\n",
    "all_sentences = []\n",
    "\n",
    "# Read each text file, split into sentences, and store\n",
    "for txt_file in txt_files:\n",
    "    file_path = os.path.join(directory_path, txt_file)\n",
    "    with open(file_path, \"r\") as file:\n",
    "        text = file.read()\n",
    "        sentences = sent_tokenize(text)\n",
    "        all_sentences.extend(sentences)\n",
    "\n",
    "# Print the first few sentences as an example\n",
    "print(all_sentences[:10])  # Print first 10 sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Embedding for the text using FastEmbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastembed import TextEmbedding\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Initialize the TextEmbedding model\n",
    "embedding_model = TextEmbedding(model_name=\"BAAI/bge-base-en\", cache_dir=\"./embeddings\")\n",
    "\n",
    "def embed_documents(documents):\n",
    "    embeddings = []\n",
    "    for document in documents:\n",
    "        # Embed document using FastEmbed\n",
    "        embedding = np.array(list((embedding_model.embed([document]))))\n",
    "        \n",
    "        # Append the embedding to the list of embeddings\n",
    "        embeddings.append(embedding)\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# Define the documents\n",
    "documents = all_sentences\n",
    "\n",
    "# Perform embedding generation\n",
    "embeddings = embed_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = [sublist[0] for sublist in embeddings]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting Qdrant-Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "from qdrant_client.http.models import PointStruct\n",
    "\n",
    "client = QdrantClient(\n",
    "    url=\"https://c065099d-b51c-4e03-b680-646b177fc993.us-east4-0.gcp.cloud.qdrant.io:6333\", \n",
    "    api_key=\"LojX3SvBqFF93pknBnlK8J5QbVR98wWzypfg7UTr4lyhdeakTYLsSA\",\n",
    "    https=True,\n",
    ")\n",
    "collection_name = 'RAG-Usage-Example'\n",
    "client.recreate_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(size=768, distance=Distance.COSINE),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CollectionsResponse(collections=[CollectionDescription(name='fastembed_collection'), CollectionDescription(name='RAG-Usage-Example')])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.get_collections()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading Embedding to Qdrant Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.upload_points(\n",
    "    collection_name=collection_name,\n",
    "    points=[\n",
    "        PointStruct(\n",
    "            id=idx,\n",
    "            vector=vector.tolist(),\n",
    "            payload={\"text\": text}\n",
    "        )\n",
    "        for idx, (vector, text) in enumerate(zip(embeddings, documents))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a RAG System with OpenaI for any Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary:\n",
      "The main idea of the text is to discuss different aspects related to the ethical and social risks associated with language models, including exploring adversarial system messages, safety in dialogue systems, measuring model behaviors in mimicking human falsehoods, and assessing the potential existential risks posed by power-seeking AI.\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from qdrant_client import QdrantClient\n",
    "from openai import OpenAI\n",
    "OpenAI_client = OpenAI(api_key='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')\n",
    "\n",
    "# Function to generate completion from prompt\n",
    "def generate_completion(prompt):\n",
    "    completion = OpenAI_client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are assisting in answering a question.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "# Function to embed Queries\n",
    "def embed_query(Question):\n",
    "    return np.array(list(embedding_model.embed([Question])))\n",
    "\n",
    "\n",
    "# Initialize Qdrant Client\n",
    "client = QdrantClient(\n",
    "    url=\"https://c065099d-b51c-4e03-b680-646b177fc993.us-east4-0.gcp.cloud.qdrant.io:6333\", \n",
    "    api_key=\"LojX3SvBqFF93pknBnlK8J5QbVR98wWzypfg7UTr4lyhdeakTYLsSA\",\n",
    "    https=True,\n",
    ")\n",
    "\n",
    "Question = \"Can AI Models be hacked?\"\n",
    "query_embeddings = embed_query(Question)\n",
    "\n",
    "collection_name = 'RAG-Usage-Example'\n",
    "all_text = \"\"\n",
    "\n",
    "# Retrieve all hits and concatenate texts into a single prompt\n",
    "for query_embedding in query_embeddings:\n",
    "    query_vector: List[np.ndarray] = list(query_embedding)\n",
    "    hits = client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_vector,\n",
    "        limit=8 \n",
    "    )\n",
    "\n",
    "    for hit in hits:\n",
    "        text = hit.payload[\"text\"]\n",
    "        all_text += text + \"\\n\\n\"\n",
    "\n",
    "# Generate completion using all texts as a single prompt\n",
    "prompt = f\"Given the following text, answer the following question:\\n\\n{all_text}\\n\\nQuestion: What is the main idea of the text?\\n\\nAnswer:\"\n",
    "completion = generate_completion(prompt)\n",
    "\n",
    "print(\"Generated Response:\")\n",
    "print(completion)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
