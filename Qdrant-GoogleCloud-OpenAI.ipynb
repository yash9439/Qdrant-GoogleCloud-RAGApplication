{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Text from PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "import numpy as np\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    reader = PdfReader(pdf_path)\n",
    "    extracted_text = \"\"\n",
    "    for page in reader.pages:\n",
    "        extracted_text += page.extract_text()\n",
    "    return extracted_text\n",
    "\n",
    "def extract_text_from_pdfs_in_directory(directory):\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(directory, filename)\n",
    "            extracted_text = extract_text_from_pdf(pdf_path)\n",
    "            txt_filename = os.path.splitext(filename)[0] + \".txt\"\n",
    "            txt_filepath = os.path.join(directory, txt_filename)\n",
    "            with open(txt_filepath, \"w\") as txt_file:\n",
    "                txt_file.write(extracted_text)\n",
    "\n",
    "# Specify the directory containing PDF files\n",
    "directory_path = \"Docs/\"\n",
    "\n",
    "# Extract text from PDFs in the directory and save as text files\n",
    "extract_text_from_pdfs_in_directory(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "directory_path = \"Docs\"\n",
    "\n",
    "# List all .txt files in the directory\n",
    "txt_files = [file for file in os.listdir(directory_path) if file.endswith('.txt')]\n",
    "\n",
    "# List to store sentences from all files\n",
    "all_sentences = []\n",
    "\n",
    "# Read each text file, split into sentences, and store\n",
    "for txt_file in txt_files:\n",
    "    file_path = os.path.join(directory_path, txt_file)\n",
    "    with open(file_path, \"r\") as file:\n",
    "        text = file.read()\n",
    "        sentences = sent_tokenize(text)\n",
    "        all_sentences.extend(sentences)\n",
    "\n",
    "# Print the first few sentences as an example\n",
    "print(all_sentences[:10])  # Print first 10 sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Embedding for the text using FastEmbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastembed import TextEmbedding\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Initialize the TextEmbedding model\n",
    "embedding_model = TextEmbedding(model_name=\"BAAI/bge-base-en\", cache_dir=\"./embeddings\")\n",
    "\n",
    "def embed_documents(documents):\n",
    "    embeddings = []\n",
    "    for document in documents:\n",
    "        # Embed document using FastEmbed\n",
    "        embedding = np.array(list((embedding_model.embed([document]))))\n",
    "        \n",
    "        # Append the embedding to the list of embeddings\n",
    "        embeddings.append(embedding)\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# Define the documents\n",
    "documents = all_sentences\n",
    "\n",
    "# Perform embedding generation\n",
    "embeddings = embed_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = [sublist[0] for sublist in embeddings]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting Qdrant-Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "from qdrant_client.http.models import PointStruct\n",
    "\n",
    "client = QdrantClient(\n",
    "    url=\"https://c065099d-b51c-4e03-b680-646b177fc993.us-east4-0.gcp.cloud.qdrant.io:6333\", \n",
    "    api_key=\"[Qdrant-API-Key]\",\n",
    "    https=True,\n",
    ")\n",
    "collection_name = 'RAG-Usage-Example'\n",
    "client.recreate_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(size=768, distance=Distance.COSINE),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.get_collections()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading Embedding to Qdrant Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.upload_points(\n",
    "    collection_name=collection_name,\n",
    "    points=[\n",
    "        PointStruct(\n",
    "            id=idx,\n",
    "            vector=vector.tolist(),\n",
    "            payload={\"text\": text}\n",
    "        )\n",
    "        for idx, (vector, text) in enumerate(zip(embeddings, documents))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a RAG System with OpenaI for any Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using OpenAI API (Gradio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from qdrant_client import QdrantClient\n",
    "from openai import OpenAI\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "\n",
    "OpenAI_client = OpenAI(api_key='[OpenAI-API-Key]')\n",
    "\n",
    "# Function to generate completion from prompt\n",
    "def generate_completion(prompt):\n",
    "    completion = OpenAI_client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are assisting in answering a question.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "# Function to embed Queries\n",
    "def embed_query(Question):\n",
    "    return np.array(list(embedding_model.embed([Question])))\n",
    "\n",
    "# Initialize Qdrant Client\n",
    "client = QdrantClient(\n",
    "    url=\"https://c065099d-b51c-4e03-b680-646b177fc993.us-east4-0.gcp.cloud.qdrant.io:6333\",\n",
    "    api_key=\"[Qdrant-API-Key]\",\n",
    "    https=True,\n",
    ")\n",
    "\n",
    "def generate_response(Question):\n",
    "    query_embeddings = embed_query(Question)\n",
    "    collection_name = 'RAG-Usage-Example'\n",
    "    all_text = \"\"\n",
    "\n",
    "    # Retrieve all hits and concatenate texts into a single prompt\n",
    "    for query_embedding in query_embeddings:\n",
    "        query_vector: List[np.ndarray] = list(query_embedding)\n",
    "        hits = client.search(\n",
    "            collection_name=collection_name,\n",
    "            query_vector=query_vector,\n",
    "            limit=8\n",
    "        )\n",
    "        for hit in hits:\n",
    "            text = hit.payload[\"text\"]\n",
    "            all_text += text + \"\\n\\n\"\n",
    "\n",
    "    # Generate completion using all texts as a single prompt\n",
    "    prompt = f\"Given the following text, answer the following question:\\n\\n{all_text}\\n\\nQuestion: What is the main idea of the text?\\n\\nAnswer:\"\n",
    "    completion = generate_completion(prompt)\n",
    "    return completion\n",
    "\n",
    "# Set up the Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=generate_response,\n",
    "    inputs=[gr.Textbox(label=\"Question\")],\n",
    "    outputs=[gr.Textbox(label=\"Generated Response\")],\n",
    "    title=\"RAG with Qdrant, FastEmbed and OpenAI\",\n",
    "    description=\"Enter a question and get a generated response based on the retrieved text.\",\n",
    ")\n",
    "\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Google Gemini API (Gradio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U google-generativeai\n",
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from qdrant_client import QdrantClient\n",
    "import google.generativeai as genai\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "\n",
    "genai.configure(api_key=\"[Google-API-Key]\")\n",
    "model = genai.GenerativeModel('gemini-pro')\n",
    "\n",
    "# Function to generate completion from prompt\n",
    "def generate_completion(prompt):\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n",
    "\n",
    "# Function to embed Queries\n",
    "def embed_query(Question):\n",
    "    return np.array(list(embedding_model.embed([Question])))\n",
    "\n",
    "# Initialize Qdrant Client\n",
    "client = QdrantClient(\n",
    "    url=\"https://c065099d-b51c-4e03-b680-646b177fc993.us-east4-0.gcp.cloud.qdrant.io:6333\",\n",
    "    api_key=\"[Qdrant-API-Key]\",\n",
    "    https=True,\n",
    ")\n",
    "\n",
    "def generate_response(Question):\n",
    "    query_embeddings = embed_query(Question)\n",
    "    collection_name = 'RAG-Usage-Example'\n",
    "    all_text = \"\"\n",
    "\n",
    "    # Retrieve all hits and concatenate texts into a single prompt\n",
    "    for query_embedding in query_embeddings:\n",
    "        query_vector: List[np.ndarray] = list(query_embedding)\n",
    "        hits = client.search(\n",
    "            collection_name=collection_name,\n",
    "            query_vector=query_vector,\n",
    "            limit=50\n",
    "        )\n",
    "        for hit in hits:\n",
    "            text = hit.payload[\"text\"]\n",
    "            all_text += text + \"\\n\\n\"\n",
    "\n",
    "    # Generate completion using all texts as a single prompt\n",
    "    prompt = f\"Given the following text, answer the following question:\\n\\n{all_text}\\n\\nQuestion: What is the main idea of the text?\\n\\nAnswer:\"\n",
    "    completion = generate_completion(prompt)\n",
    "    return completion\n",
    "\n",
    "# Set up the Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=generate_response,\n",
    "    inputs=[gr.Textbox(label=\"Question\")],  # Pass input as a list\n",
    "    outputs=[gr.Textbox(label=\"Generated Response\")],  # Pass output as a list\n",
    "    title=\"RAG with Qdrant, FastEmbed and Gemini\",\n",
    "    description=\"Enter a question and get a generated response based on the retrieved text.\",\n",
    ")\n",
    "\n",
    "iface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
